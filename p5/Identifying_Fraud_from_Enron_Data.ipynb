{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Fraud from Enron Data\n",
    "\n",
    "## Objective\n",
    "\n",
    "This project will use machine learning to attempt to identify persons of interest from the Enron financial scandel. Persons of Interest (POIs) were defined at the onset by Udacity using an article from USA Today <cite data-cite=\"6112171/BAJ6H3FC\"></cite>. Two data resources were provided by Udacity for identifying POIs. The first is the Enron Email Datset maintained by Carnegie Mellon University <cite data-cite=\"6112171/GLPDFEY4\"></cite>. The email dataset contains a corpus of close to 500,000 email messages from Enron's servers. The second is a dataset created by Udacity with a collection of useful information for the task. It contains both financial features as well as aggregate information about the email dataset. \n",
    "\n",
    "Machine learning excels at this type of task because it can be used to process large amounts of information in a way that would be time consuming if done by hand. Machine learning algorithms use statistical methods to *learn* from data and can then make inferences based on the relationships they have found. Although developing the algorithms takes a considerable amount of care and effort in itself, when completed they can be used to process large datasets that would take days for a human to review within seconds.\n",
    "\n",
    "### Data Exploration\n",
    "The data was loaded into pandas dataframes to simplify exploration and visualization. There were 146 entries and 21 features. 18 (12%) of the entries were identified as persons of interest.\n",
    "\n",
    "The data was created from two sources. The financial information came from a document attributed to Findlaw.com. Whereas the email information was compiled by aggregating information from the Enron Email corpus. For this reason it made sense to divide the data when looking for trends and similarities between the features.\n",
    "\n",
    "### Outlier Investigation\n",
    "To search for outliers I screened for any values with a zscore above 3. I also visualized the results to get a feel for the distributions. The largest outliers were created by the presence of a 'TOTAL' entry in the financial data. This entry prompted me to seach through the Findlaw.com document for other unusual items. I also found a travel agency that needed to be removed.\n",
    "\n",
    "Plotting the data showed that most of the financial features were positively skewed. This made sense, considering the presense of executives and directors in the data. Their compensation was far beyond the rest of the employees. They also receive unique types of compensation such as restricted stock and director fees. These outliers did not represent errors were kept.\n",
    "\n",
    "The email data also showed some outliers, which was expected. Individuals who regularly contacted POIs should stand out from those who didn't because the POIs only make up a small portion of the email corpus. However, the persons who were identified as outliers for non-POI related fields represent a risk in data leakage. I did some searching and confirmed that these individuals held high level positions at Enron. It is likely they had a higher presense in the email corpus because they were the focus of investigations. This created a potential problem where the focus of the investigation could leak in as a factor in identifying POIs, which I planned to look out for in feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "### Create new features\n",
    "One theory I had was that incidents like the Enron scandal are created by systemic problems within companies. For example, in the case of Enron employee compensation could have been structured in a way that rewarded bad actors.\n",
    "\n",
    "I created a two new features to search for compensation schemes that might have been used as performance incentives. The idea being that if these incentives were being used to reward the most competitive employees, the 'cheaters' might have risen to the top.\n",
    "1. bonus_ratio: bonus / salary\n",
    "    - How large was the bonus in relation to salary?\n",
    "2. stock_ratio: total_stock_value / total_payments\n",
    "    - Did any employees receive large amounts of stock in comparison with more standard compensation payments?\n",
    "\n",
    "I also created two email ratios to try to prevent the potential data leakage I identified during outlier investigation. These features normalize the email interactions with POIs so that they are less influenced by the total number of emails for each person in the database.\n",
    "1. from_this_person_to_poi_ratio: from_this_person_to_poi / from_messages\n",
    "2. from_poi_to_this_person_ratio: from_poi_to_this_person / to_messages\n",
    "\n",
    "After creating these variables there were 23 features in the dataset, not counting 'poi' and 'email_address'. Email addresses were removed as they had no value for the approach I chose.\n",
    "\n",
    "### Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction was an important part of the process because of the relatively small number of entries in the dataset. Using too many features with a small dataset increases the chance of overfitting. In addition, the number of financial features with sparse data made performing cross validation difficult. Data is considered sparse when it has a high percentage of zeros for values. This can cause a cross validator to return drastically different results depending on where the division of training and validation sets is made, and can consequently lead to overfitting.\n",
    "\n",
    "To reduce these problems I used sklearn's `f_classif` function to identify the features with the lowest p-values. I also examined features with high percentages of sparse data. After inspection I decided to drop all of the features with greater than 60% sparse data and p-values over .05. As a last manual step I removed two features which had a high correlation (>.9) to other features in the dataset. These were total_stock_value and total_payments. Removing these was a natural choice, since they are composites of the other features and do not add information.\n",
    "\n",
    "With this filtered list I was able to perform a grid search to select the features that I wanted to use. The grid search utilized a pipeline to perform an exhaustive search of all the features against several algorithms. The first step of the pipeline was to standardize the data. Standardization was necessary to scale the data so that it was appropriate for all of the algorithms. Sklearn's `MinMaxScaler` was used because of its ability to handle outliers <cite data-cite=\"6112171/C25VRN7E\"></cite>. Features were selected using `SelectKBest` to pick the best k features based on thier ANOVA F-values. A total of 6 features made the final selction. Their F-values are in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>F-value</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>exercised_stock_options</th>\n",
    "      <td>25.097542</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>bonus</th>\n",
    "      <td>21.060002</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>salary</th>\n",
    "      <td>18.575703</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>from_this_person_to_poi_ratio</th>\n",
    "      <td>16.641707</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>bonus_ratio</th>\n",
    "      <td>10.955627</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>long_term_incentive</th>\n",
    "      <td>10.072455</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Selection\n",
    "\n",
    "Four algoritms were tested as part of the grid search performed for feature selection. They were `LinearSVC`, `KNeighborsClassifier`, `DecisionTreeClassifier` and `AdaBoostClassifier`. The grid search identified the best classfier based on the maximum f1_score that was obtained from all of the tests.\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>f1_score</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>LinearSVC</th>\n",
    "      <td>0.21133</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>KNeighborsClassifier</th>\n",
    "      <td>0.302</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>DecisionTreeClassifier</th>\n",
    "      <td>0.37257</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>AdaBoostClassifier</th>\n",
    "      <td>0.33505</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "DecisionTreeClassifier scored the highest in the gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Tuning\n",
    "\n",
    "Algorithms often have a variety of paramaters that can be tuned to optimize performance. Proper tuning is important in balancing the bias-variance tradeoff of the algorithm <cite data-cite=\"6112171/A7GRXU5P\"></cite>. If the algorithm is tuned too aggressively it will perform well on the training set, but poorly on new data. Conversely, if the algorithm is tuned too conservitively it will perform equally on training and test data, but at the same time not acheive all of its potential accuracy.\n",
    "\n",
    "To tune the DecisionTreeClassifier I created a second gridsearch to test the following parameters and options:\n",
    "- class_weight:\n",
    "    - None, balanced\n",
    "- criterion:\n",
    "    - Gini, Entropy\n",
    "- splitter:\n",
    "    - Best, Random\n",
    "- max_depth:\n",
    "    - None, 7, 6, 5, 4\n",
    "\n",
    "The best score was obtained using the settings 'balanced', 'Entropy', 'Best', and 4. The effect was to raise the f1 score to 0.52538."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Validation is the process of testing algorithm performance and parameter tuning. A classic mistake is to test an algorithm's performance on the data it was trained and optimized with. It usually results in very high test scores but poor performance on unknown data. Choosing a robust validation strategy is necessary to ensure that a machine learning algorithm will be able to make predictions when new data is introduced.\n",
    "\n",
    "I used the same `StratifiedShuffleSplit` cross-validation tool as in the tester.py script provided by Udacity. It lends itself well to this project because of the small size of the dataset. It creates random splits of test and train data while maintaining the ratio of classes so that each split is representative of the whole dataset. The random splitting means that it is possible that data is reused between different splits, but it also allows it to create many more combinations for testing than would be possible if data reuse was not allowed. I implemented the approach from within the `GridSearchCV` cross-validators I used for model selection and tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "I used the f1 score as the performance objective of model selection and tuning. It is a better tool than measuring accuracy because of the imbalanced nature of the data. It is the weighted average of precision and recall, which are coincidentally the grading metrics of the project. Precision measures how accurate the model is when it makes a prediction, whereas recall measures the ability of the model to make the correct prediction given a class. A model can have a high precision for a class by being conservative in how it predicts that class, while at the same time be penalized for not predicting the class often enough when it occurs. The f1 score balances this tradeoff.\n",
    "\n",
    "The tester.py returned the following metrics for the final algorithm:\n",
    "- Accuracy: 0.765\n",
    "- Precision: 0.34135\n",
    "- Recall: 0.5675\n",
    "- F1 score: 0.42629"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"cite2c-biblio\"></div>"
   ]
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "6112171/6G4JA9CK": {
     "URL": "http://scikit-learn.org/stable/modules/preprocessing.html",
     "accessed": {
      "day": 30,
      "month": 5,
      "year": 2018
     },
     "id": "6112171/6G4JA9CK",
     "title": "4.3. Preprocessing data — scikit-learn 0.19.1 documentation",
     "type": "webpage"
    },
    "6112171/A7GRXU5P": {
     "URL": "https://en.wikipedia.org/w/index.php?title=Bias%E2%80%93variance_tradeoff&oldid=843449040",
     "abstract": "In statistics and machine learning, the bias–variance tradeoff is the property of a set of predictive models whereby models with a lower bias in parameter estimation have a higher variance of the parameter estimates across samples, and vice versa. The bias–variance dilemma or problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:\nThe bias is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\nThe variance is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting).\nThe bias–variance decomposition is a way of analyzing a learning algorithm's expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself.\nThis tradeoff applies to all forms of supervised learning: classification, regression (function fitting), and structured output learning. It has also been invoked to explain the effectiveness of heuristics in human learning.",
     "accessed": {
      "day": 30,
      "month": 5,
      "year": 2018
     },
     "container-title": "Wikipedia",
     "id": "6112171/A7GRXU5P",
     "issued": {
      "day": 29,
      "month": 5,
      "year": 2018
     },
     "language": "en",
     "note": "Page Version ID: 843449040",
     "title": "Bias–variance tradeoff",
     "type": "entry-encyclopedia"
    },
    "6112171/BAJ6H3FC": {
     "URL": "http://usatoday30.usatoday.com/money/industries/energy/2005-12-28-enron-participants_x.htm",
     "accessed": {
      "day": 28,
      "month": 5,
      "year": 2018
     },
     "id": "6112171/BAJ6H3FC",
     "title": "USATODAY.com - A look at those involved in the Enron scandal",
     "type": "webpage"
    },
    "6112171/C25VRN7E": {
     "URL": "http://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py",
     "accessed": {
      "day": 30,
      "month": 5,
      "year": 2018
     },
     "id": "6112171/C25VRN7E",
     "title": "Compare the effect of different scalers on data with outliers — scikit-learn 0.19.1 documentation",
     "type": "webpage"
    },
    "6112171/GLPDFEY4": {
     "URL": "https://www.cs.cmu.edu/~enron/",
     "accessed": {
      "day": 28,
      "month": 5,
      "year": 2018
     },
     "id": "6112171/GLPDFEY4",
     "title": "Enron Email Dataset",
     "type": "webpage"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
